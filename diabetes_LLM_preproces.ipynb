{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPH7Ugpp78sTSHgNkgfmaSL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veselm73/BP/blob/main/diabetes_LLM_preproces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oen-SNbpa6vv",
        "outputId": "c196373b-6cc5-4055-c086-f3772cd7dfa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas tqdm scikit-learn nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "\n",
        "class FastTFIDFKeywordExtractor:\n",
        "    def __init__(self, top_n=5, custom_stopwords=None):\n",
        "        self.top_n = top_n\n",
        "        self.tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "        self.custom_stopwords = custom_stopwords or {\n",
        "            \"type\", \"unspecified\", \"complication\", \"disease\", \"stage\",\n",
        "            \"syndrome\", \"disorder\", \"with\", \"without\", \"of\"\n",
        "        }\n",
        "\n",
        "    def preprocess_texts(self, texts):\n",
        "        return [\n",
        "            re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", str(text).lower())\n",
        "            for text in texts\n",
        "        ]\n",
        "\n",
        "    def extract_keywords_batch(self, corpus):\n",
        "        preprocessed = self.preprocess_texts(corpus)\n",
        "\n",
        "        # Fit TF-IDF\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            tokenizer=self.tokenizer.tokenize,\n",
        "            stop_words=\"english\"\n",
        "        )\n",
        "        tfidf_matrix = vectorizer.fit_transform(preprocessed)\n",
        "        feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "        results = []\n",
        "        for i in tqdm(range(tfidf_matrix.shape[0]), desc=\"Extracting top keywords\"):\n",
        "            row = tfidf_matrix.getrow(i)\n",
        "            scores = row.data\n",
        "            indices = row.indices\n",
        "\n",
        "            sorted_idx = np.argsort(scores)[::-1]\n",
        "            sorted_terms = feature_names[indices[sorted_idx]]\n",
        "\n",
        "            seen_stems = set()\n",
        "            keywords = []\n",
        "\n",
        "            for word in sorted_terms:\n",
        "                stem = self.stemmer.stem(word)\n",
        "                if (\n",
        "                    word not in self.stop_words and\n",
        "                    word not in self.custom_stopwords and\n",
        "                    stem not in seen_stems\n",
        "                ):\n",
        "                    seen_stems.add(stem)\n",
        "                    keywords.append(word)\n",
        "                if len(keywords) >= self.top_n:\n",
        "                    break\n",
        "\n",
        "            results.append(\" \".join(keywords))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_and_replace(self, df, columns):\n",
        "        df_cleaned = df.copy()\n",
        "        for col in columns:\n",
        "            if col not in df_cleaned.columns:\n",
        "                print(f\"Column '{col}' not found.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nExtracting keywords from: {col}\")\n",
        "            texts = df_cleaned[col].astype(str).tolist()\n",
        "            df_cleaned[col] = self.extract_keywords_batch(texts)\n",
        "\n",
        "        return df_cleaned\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUUpH9hIrZ1S",
        "outputId": "2c513801-525b-482b-985f-40484493b53e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"https://kmlinux.fjfi.cvut.cz/~veselm73/diabetes_preprocessed.csv\"\n",
        "df = pd.read_csv(file_path, low_memory=False)"
      ],
      "metadata": {
        "id": "7GgFQZ5fczcC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_columns = [\n",
        "    \"diag_2_desc\",\n",
        "    \"diag_3_desc\",\n",
        "    \"primary_diag_desc\"\n",
        "]\n",
        "\n",
        "extractor = FastTFIDFKeywordExtractor(top_n=5)\n",
        "\n",
        "df_cleaned = extractor.extract_and_replace(df, text_columns)\n",
        "\n",
        "df_cleaned.to_csv(\"diabetes_short_diag.csv\", index=False)\n",
        "print(len(df_cleaned))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuOoZYn9kujR",
        "outputId": "10e38e47-8ad8-40f2-84be-1cad28349d57"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting keywords from: diag_2_desc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "Extracting top keywords: 100%|██████████| 101766/101766 [00:24<00:00, 4194.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting keywords from: diag_3_desc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "Extracting top keywords: 100%|██████████| 101766/101766 [00:15<00:00, 6412.82it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting keywords from: primary_diag_desc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting top keywords: 100%|██████████| 101766/101766 [00:15<00:00, 6650.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzbM3vUYt1vD"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}